import os, time, traceback, requests, urllib.parse, json
from typing import List, Dict
from dotenv import load_dotenv
from openai import OpenAI
from datetime import datetime

load_dotenv()

SERPAPI_KEY = os.getenv("SERPAPI_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
SERPAPI_HL = os.getenv("SERPAPI_HL", "ar")
SERPAPI_GL = os.getenv("SERPAPI_GL", "")
NEWS_AGENCIES = [d.strip() for d in os.getenv("NEWS_AGENCIES", "aljazeera.net,una-oic.org,bbc.com").split(",") if d.strip()]

if not SERPAPI_KEY or not OPENAI_API_KEY:
    raise RuntimeError("‚ö†Ô∏è ÿ±ÿ¨ÿßÿ°Ÿã ÿ∂ÿπ SERPAPI_KEY Ÿà OPENAI_API_KEY ŸÅŸä .env")

client = OpenAI(api_key=OPENAI_API_KEY)

def _lang_hint_from_claim(text: str) -> str:
    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "Detect the input language and return ONLY its ISO 639-1 code (like ar, en, fr, es, de)."},
                {"role": "user", "content": text.strip()},
            ],
            temperature=0.0,
            max_tokens=5
        )
        lang = (resp.choices[0].message.content or "").strip().lower()
        if len(lang) == 2:
            return lang
    except Exception:
        pass

    # fallback
    ar_count = sum(1 for ch in text if '\u0600' <= ch <= '\u06FF')
    ratio = ar_count / max(1, len(text))
    return "ar" if ratio >= 0.15 else "en"

def _fetch_serp(query: str, extra: Dict | None = None, num: int = 10) -> List[Dict]:
    url = "https://serpapi.com/search.json"
    params = {
        "q": query,
        "api_key": SERPAPI_KEY,
        "hl": SERPAPI_HL,
        "gl": SERPAPI_GL,
        "num": num
    }
    if extra:
        params.update(extra)
    try:
        print(f"üîç Fetching: {query}")
        r = requests.get(url, params=params, timeout=20)
        r.raise_for_status()
        data = r.json()
        results = []
        for it in data.get("organic_results", []):
            results.append({
                "title": it.get("title") or "",
                "snippet": it.get("snippet") or (it.get("snippet_highlighted_words", [""]) or [""])[0],
                "link": it.get("link") or it.get("displayed_link") or "",
            })
        print(f"‚úÖ Found {len(results)} results for query: {query}")
        return [r for r in results if r["title"] or r["snippet"] or r["link"]]
    except Exception as e:
        print("‚ùå Error fetching from SerpAPI:", e)
        return []

FACT_PROMPT_SYSTEM = (
    "You are a rigorous fact-checking assistant. Use ONLY the sources provided below.\n"
    "- If evidence is insufficient, conflicting, or off-topic, the verdict must be: Uncertain.\n"
    "- Prefer official catalogs and reputable agencies over blogs or social posts.\n"
    "- Match the claim's date/place/magnitude when relevant; do not infer beyond the given sources.\n\n"

    "LANGUAGE POLICY:\n"
    "- You MUST respond **entirely** in the language specified by LANG_HINT.\n"
    "- Do NOT switch to another language or translate.\n"
    "- Examples:\n"
    "   ‚Ä¢ If LANG_HINT = 'fr' ‚Üí respond fully in French.\n"
    "   ‚Ä¢ If LANG_HINT = 'ar' ‚Üí respond fully in Arabic.\n"
    "   ‚Ä¢ If LANG_HINT = 'en' ‚Üí respond fully in English.\n"
    "   ‚Ä¢ If LANG_HINT = 'es' ‚Üí respond fully in Spanish.\n"
    "   ‚Ä¢ If LANG_HINT = 'cs' ‚Üí respond fully in Czech.\n\n"

    "FORMAT RULES:\n"
    "‚Ä¢ You MUST write all free-text fields strictly in LANG_HINT language.\n"
    "‚Ä¢ JSON keys must remain EXACTLY as: \"ÿßŸÑÿ≠ÿßŸÑÿ©\", \"talk\", \"sources\" (do not translate keys).\n"
    "‚Ä¢ The value of \"ÿßŸÑÿ≠ÿßŸÑÿ©\" must be localized according to LANG_HINT:\n"
    "   - Arabic: ÿ≠ŸÇŸäŸÇŸä / ŸÉÿßÿ∞ÿ® / ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ\n"
    "   - English: True / False / Uncertain\n"
    "   - French: Vrai / Faux / Incertain\n"
    "   - Spanish: Verdadero / Falso / Incierto\n"
    "   - Czech: Pravda / Nepravda / Nejist√©\n"
    "‚Ä¢ Inside \"talk\": end the explanation with a localized label for sources:\n"
    "   - Arabic: ÿ±Ÿàÿßÿ®ÿ∑ ÿ±ÿ¶Ÿäÿ≥Ÿäÿ©:\n"
    "   - English: Key sources:\n"
    "   - French: Sources principales:\n"
    "   - Spanish: Fuentes principales:\n"
    "   - Czech: Kl√≠ƒçov√© zdroje:\n\n"

    "RESPONSE FORMAT (JSON ONLY ‚Äî no extra text):\n"
    "{\n"
    '  \"ÿßŸÑÿ≠ÿßŸÑÿ©\": \"<Localized verdict>\",\n'
    '  \"talk\": \"<Explanation paragraph ~350 words in LANG_HINT>\",\n'
    '  \"sources\": [ {\"title\": \"<title>\", \"url\": \"<url>\"}, ... ]\n'
    "}\n\n"

    "FINAL RULES:\n"
    "1) Output STRICTLY valid JSON (UTF-8). No extra commentary before or after.\n"
    "2) If the claim is Uncertain ‚Üí keep 'sources' as an empty array [].\n"
    "3) If the claim is True ‚Üí include ALL confirming sources (no fixed limit).\n"
    "4) Do not fabricate URLs or titles; use only provided sources.\n"
)


def check_fact_simple(claim_text: str, k_sources: int = 5) -> dict:
    try:
        print(f"üß† Fact-checking: {claim_text}")
        lang = _lang_hint_from_claim(claim_text)

        results = []
        for domain in NEWS_AGENCIES:
            domain_results = _fetch_serp(f"{claim_text} site:{domain}", extra={"hl": lang} if lang else None, num=2)
            results += domain_results
        google_results = _fetch_serp(claim_text, extra={"hl": lang} if lang else None, num=k_sources)
        results += google_results

        print(f"üîé Total combined results: {len(results)}")

        if not results:
            no_results_by_lang = {
                "ar": "ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿ≠ÿ´.",
                "en": "No search results were found.",
                "fr": "Aucun r√©sultat de recherche trouv√©.",
                "es": "No se encontraron resultados de b√∫squeda.",
                "cs": "Nebyly nalezeny ≈æ√°dn√© v√Ωsledky vyhled√°v√°n√≠.",
                "de": "Es wurden keine Suchergebnisse gefunden.",
                "tr": "Arama sonu√ßlarƒ± bulunamadƒ±.",
                "ru": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.",
            }
            return {"case": "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ", "talk": no_results_by_lang.get(lang, no_results_by_lang["en"]), "sources": []}

        def clip(s: str, n: int) -> str:
            return s.strip() if len(s) <= n else s[:n] + "‚Ä¶"

        context = "\n\n---\n\n".join(
            f"ÿπŸÜŸàÿßŸÜ: {clip(r['title'], 100)}\nŸÖŸÑÿÆÿµ: {clip(r['snippet'], 200)}\nÿ±ÿßÿ®ÿ∑: {r['link']}"
            for r in results
        )

        system_prompt = FACT_PROMPT_SYSTEM.replace("LANG_HINT", lang)
        user_msg = f"""
LANG_HINT: {lang}
CURRENT_DATE: {datetime.now().strftime('%Y-%m-%d')}

ÿßŸÑÿßÿØÿπÿßÿ°:
{claim_text}

ÿßŸÑÿ≥ŸäÿßŸÇ:
{context}
""".strip()

        print("üì§ Sending prompt to OpenAI")
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.2,
        )
        answer = (resp.choices[0].message.content or "").strip()
        if answer.startswith("```"):
            answer = answer.strip("` \n")
            if answer.lower().startswith("json"):
                answer = answer[4:].strip()

        parsed = json.loads(answer)

        case = parsed.get("ÿßŸÑÿ≠ÿßŸÑÿ©", "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ")
        talk = parsed.get("talk", "")
        sources = parsed.get("sources", [])

        uncertain_terms = {
            "ar": {"ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ"},
            "en": {"uncertain"},
            "fr": {"incertain"},
            "es": {"incierto"},
            "cs": {"nejist√©", "nejiste", "nejist√°"},
            "de": {"unsicher"},
            "tr": {"belirsiz"},
            "ru": {"–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ", "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π"},
        }
        lowered = case.strip().lower()
        if lowered in {t for s in uncertain_terms.values() for t in s}:
            sources = []

        return {"case": case, "talk": talk, "sources": sources}

    except Exception as e:
        print("‚ùå Error:", traceback.format_exc())
        error_by_lang = {
            "ar": "‚ö†Ô∏è ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ÿßŸÑÿ™ÿ≠ŸÇŸÇ.",
            "en": "‚ö†Ô∏è An error occurred during fact-checking.",
            "fr": "‚ö†Ô∏è Une erreur s'est produite lors de la v√©rification des faits.",
            "es": "‚ö†Ô∏è Se produjo un error durante la verificaci√≥n de hechos.",
            "cs": "‚ö†Ô∏è Bƒõhem ovƒõ≈ôov√°n√≠ fakt≈Ø do≈°lo k chybƒõ.",
            "de": "‚ö†Ô∏è Bei der Faktenpr√ºfung ist ein Fehler aufgetreten.",
            "tr": "‚ö†Ô∏è Doƒürulama sƒ±rasƒ±nda bir hata olu≈ütu.",
            "ru": "‚ö†Ô∏è –í–æ –≤—Ä–µ–º—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.",
        }
        try:
            lang = _lang_hint_from_claim(claim_text)
        except Exception:
            lang = "en"
        return {"case": "ÿ∫Ÿäÿ± ŸÖÿ§ŸÉÿØ", "talk": error_by_lang.get(lang, error_by_lang["en"]), "sources": []}
